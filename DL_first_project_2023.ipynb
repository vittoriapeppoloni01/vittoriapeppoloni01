{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bl1nZd_VEXHk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, GRU, Dense, Input, Bidirectional, Dropout\n",
        "from keras.layers.core import  Activation\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vq-ZPdsNlPV",
        "outputId": "a8b7aa1b-bb38-4bf1-9aaf-046bd5a27017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idqkzzwYEXHq",
        "outputId": "ad351425-30d7-4a5d-f36a-1065c4764d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus length: 581888\n",
            "text[:100]:ï»¿ project gutenberg's the adventures of sherlock holmes, by arthur conan doyle  this ebook is for th\n",
            "Total chars: 72\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/Lecture 14  26042023-20230426/1661-0.txt'\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))\n",
        "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
        "print(f\"text[:100]:{text[:100]}\")\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "# char dictionaries\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Jkp26dEXHt",
        "outputId": "6bb5b4da-ae09-4f0b-a0ff-8ce665ea48c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 581848\n"
          ]
        }
      ],
      "source": [
        "# cut the text in overlapping sequences of maxlen characters\n",
        "# each time we slide of one character\n",
        "maxlen = 40\n",
        "\n",
        "sentences = [] #input sequences\n",
        "next_chars = [] #target characters\n",
        "for i in range(len(text) - maxlen):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "print(\"Number of sequences:\", len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YE8renfXEXHu"
      },
      "outputs": [],
      "source": [
        "# Creating one hot encoding\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fCyhBSiEXHv"
      },
      "source": [
        "where num_sequences is the total number of sequences, maxlen is the length of each sequence, and num_chars is the total number of unique characters in the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kNA8XvqEXHy",
        "outputId": "2f022061-cc64-4399-d748-00f524469dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(581848, 40, 72) (581848, 72)\n"
          ]
        }
      ],
      "source": [
        "print(x.shape, y.shape) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohlGE_qUEXHz"
      },
      "source": [
        "These one-hot encoded inputs and targets can be fed into the RNN for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kovco7OtEXHz"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt2qaW24EXH0"
      },
      "source": [
        "> Model 1: a single LSTM layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqE7HcVoEXH1"
      },
      "source": [
        "This model has a single LSTM layer and can capture long-term dependencies in the input text. However, it may not be able to capture complex patterns in the data that require multiple layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQoNYYY0EXH2"
      },
      "source": [
        "The model architecture is as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "del4xXN1EXH2"
      },
      "source": [
        "- The input layer takes in a sequence of characters with a maximum length of \"maxlen\" and a one-hot encoding of \"len(chars)\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4cHUvMREXH3"
      },
      "source": [
        "- The LSTM layer has 128 units, which are a type of recurrent neural network unit that is good at processing sequential data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uEcwlQsEXH3"
      },
      "source": [
        "- The output layer is a Dense layer with \"len(chars)\" units and a softmax activation function, which will output a probability distribution over the possible characters in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Dropout** is a regularization technique used in neural networks to prevent overfitting. It works by randomly dropping out (i.e., setting to zero) a certain percentage of the input units of a layer during each training iteration.\n",
        "\n",
        "The idea behind dropout is that by randomly dropping out input units, the network becomes less sensitive to the specific weights of individual neurons. This reduces the network's tendency to overfit by forcing it to learn more robust features that are useful across multiple neurons."
      ],
      "metadata": {
        "id": "R0J6_LmSpdqT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaJAR6DKEXH4",
        "outputId": "367b50e1-a6fe-4573-9166-49b554704f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dropout (Dropout)           (None, 40, 72)            0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               102912    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 72)                9288      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 112,200\n",
            "Trainable params: 112,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        Input(shape=(maxlen, len(chars))), #maximum legth of the input sequence + numer of unique characters in the input data\n",
        "        Dropout(0.2),\n",
        "        LSTM(128), #n of units in the layer\n",
        "        Dropout(0.2),\n",
        "        Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "#for check if the file already exists\n",
        "model_file = 'NCP_model.h5'\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrETEoENEXH5",
        "outputId": "7760d936-be17-4ff5-fef1-89436557cad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4546/4546 [==============================] - 27s 6ms/step - loss: 1.7102 - accuracy: 0.4868\n",
            "Epoch 2/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.7107 - accuracy: 0.4872\n",
            "Epoch 3/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.7091 - accuracy: 0.4877\n",
            "Epoch 4/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.7070 - accuracy: 0.4879\n",
            "Epoch 5/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.7058 - accuracy: 0.4878\n",
            "Epoch 6/50\n",
            "4546/4546 [==============================] - 25s 6ms/step - loss: 1.7067 - accuracy: 0.4884\n",
            "Epoch 7/50\n",
            "4546/4546 [==============================] - 27s 6ms/step - loss: 1.7032 - accuracy: 0.4888\n",
            "Epoch 8/50\n",
            "4546/4546 [==============================] - 30s 6ms/step - loss: 1.7059 - accuracy: 0.4882\n",
            "Epoch 9/50\n",
            "4546/4546 [==============================] - 29s 6ms/step - loss: 1.7017 - accuracy: 0.4896\n",
            "Epoch 10/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.7035 - accuracy: 0.4890\n",
            "Epoch 11/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.7008 - accuracy: 0.4892\n",
            "Epoch 12/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.7003 - accuracy: 0.4894\n",
            "Epoch 13/50\n",
            "4546/4546 [==============================] - 27s 6ms/step - loss: 1.7004 - accuracy: 0.4900\n",
            "Epoch 14/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6979 - accuracy: 0.4906\n",
            "Epoch 15/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6982 - accuracy: 0.4902\n",
            "Epoch 16/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6988 - accuracy: 0.4907\n",
            "Epoch 17/50\n",
            "4546/4546 [==============================] - 28s 6ms/step - loss: 1.6958 - accuracy: 0.4907\n",
            "Epoch 18/50\n",
            "4546/4546 [==============================] - 28s 6ms/step - loss: 1.6971 - accuracy: 0.4907\n",
            "Epoch 19/50\n",
            "4546/4546 [==============================] - 27s 6ms/step - loss: 1.6936 - accuracy: 0.4918\n",
            "Epoch 20/50\n",
            "4546/4546 [==============================] - 31s 7ms/step - loss: 1.6926 - accuracy: 0.4927\n",
            "Epoch 21/50\n",
            "4546/4546 [==============================] - 29s 6ms/step - loss: 1.6943 - accuracy: 0.4910\n",
            "Epoch 22/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6929 - accuracy: 0.4914\n",
            "Epoch 23/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6917 - accuracy: 0.4923\n",
            "Epoch 24/50\n",
            "4546/4546 [==============================] - 27s 6ms/step - loss: 1.6915 - accuracy: 0.4921\n",
            "Epoch 25/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6904 - accuracy: 0.4920\n",
            "Epoch 26/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6897 - accuracy: 0.4919\n",
            "Epoch 27/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6891 - accuracy: 0.4923\n",
            "Epoch 28/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6898 - accuracy: 0.4920\n",
            "Epoch 29/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6890 - accuracy: 0.4922\n",
            "Epoch 30/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6889 - accuracy: 0.4925\n",
            "Epoch 31/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6857 - accuracy: 0.4931\n",
            "Epoch 32/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6847 - accuracy: 0.4936\n",
            "Epoch 33/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6885 - accuracy: 0.4928\n",
            "Epoch 34/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6858 - accuracy: 0.4933\n",
            "Epoch 35/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6844 - accuracy: 0.4943\n",
            "Epoch 36/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6848 - accuracy: 0.4936\n",
            "Epoch 37/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6850 - accuracy: 0.4936\n",
            "Epoch 38/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6831 - accuracy: 0.4935\n",
            "Epoch 39/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6841 - accuracy: 0.4936\n",
            "Epoch 40/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6856 - accuracy: 0.4934\n",
            "Epoch 41/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6823 - accuracy: 0.4939\n",
            "Epoch 42/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6822 - accuracy: 0.4943\n",
            "Epoch 43/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6826 - accuracy: 0.4938\n",
            "Epoch 44/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6798 - accuracy: 0.4949\n",
            "Epoch 45/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6804 - accuracy: 0.4951\n",
            "Epoch 46/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6805 - accuracy: 0.4950\n",
            "Epoch 47/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6788 - accuracy: 0.4950\n",
            "Epoch 48/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6789 - accuracy: 0.4947\n",
            "Epoch 49/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6776 - accuracy: 0.4954\n",
            "Epoch 50/50\n",
            "4546/4546 [==============================] - 26s 6ms/step - loss: 1.6774 - accuracy: 0.4956\n",
            "4546/4546 [==============================] - 21s 5ms/step - loss: 1.3034 - accuracy: 0.5991\n",
            "Test loss: 1.3033686876296997\n",
            "Test accuracy: 0.5990825891494751\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "batch_size = 128 # the model will process 128 examples at a time during each training iteration.\n",
        "model.fit(x, y, \n",
        "          batch_size=batch_size, \n",
        "          epochs=epochs,\n",
        "          shuffle=True)\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x, y, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "model.save_weights(model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_6RDp-MEXH5"
      },
      "source": [
        "> Model 2: multiple LSTM hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsS8WYZaEXH6"
      },
      "source": [
        "This model has multiple LSTM hidden layers and can capture more complex patterns in the input data. However, it may be slower to train and more prone to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak5UG1e6EXH6",
        "outputId": "57d25344-de07-42af-cd01-d0fe2faf1ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 40, 256)           336896    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 40, 256)           0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 128)               197120    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 72)                9288      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 543,304\n",
            "Trainable params: 543,304\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model2 = keras.Sequential(\n",
        "    [\n",
        "        Input(shape=(maxlen, len(chars))),\n",
        "        LSTM(256, return_sequences=True),# it will output the entire sequence, which is then used as input to the second LSTM layer.\n",
        "        Dropout(0.2),\n",
        "        LSTM(128),   #second LSTM\n",
        "        Dropout(0.2),\n",
        "        Dense(len(chars), activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "model_file2 = 'NCP2_model.h5'\n",
        "'''import os\n",
        "if os.path.isfile(model_file2):\n",
        "  \n",
        "  print(\"file exists, loading\")'''\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "print(model2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC75a3GDNJ66"
      },
      "source": [
        "The **softmax function** is used to convert the outputs of the RNN into probabilities that can be used to make predictions.\n",
        "Takes as input a vector of values and outputs a vector of the same length, with each element in the output vector representing the probability of the corresponding input element belonging to a particular class. The values in the output vector are between 0 and 1, and they sum to 1, ensuring that the output represents a valid probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG-SnECzPWR2"
      },
      "source": [
        "**Cross-entropy**: measures the difference between the predicted probability distribution and the true probability distribution of the target variable. \n",
        "During training, the goal is to **minimize** the cross-entropy loss over the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZtoIYg8P6-b"
      },
      "source": [
        "**Accuracy:** number of correct predictions / total number of predictions.\n",
        "Useful when predicted class is balanced\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR0DiAohEXH7",
        "outputId": "36ab0415-7337-47de-ccbd-dd6f2370cdf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4546/4546 [==============================] - 57s 11ms/step - loss: 2.2155 - accuracy: 0.3586\n",
            "Epoch 2/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.8269 - accuracy: 0.4556\n",
            "Epoch 3/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.6816 - accuracy: 0.4943\n",
            "Epoch 4/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.5921 - accuracy: 0.5184\n",
            "Epoch 5/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.5814 - accuracy: 0.5228\n",
            "Epoch 6/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.5010 - accuracy: 0.5440\n",
            "Epoch 7/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.4637 - accuracy: 0.5541\n",
            "Epoch 8/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.4334 - accuracy: 0.5621\n",
            "Epoch 9/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.4068 - accuracy: 0.5693\n",
            "Epoch 10/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.3950 - accuracy: 0.5724\n",
            "Epoch 11/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.3957 - accuracy: 0.5722\n",
            "Epoch 12/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.6468 - accuracy: 0.5029\n",
            "Epoch 13/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.5959 - accuracy: 0.5164\n",
            "Epoch 14/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.3727 - accuracy: 0.5788\n",
            "Epoch 15/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.3402 - accuracy: 0.5870\n",
            "Epoch 16/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.3252 - accuracy: 0.5902\n",
            "Epoch 17/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.3112 - accuracy: 0.5939\n",
            "Epoch 18/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.3012 - accuracy: 0.5965\n",
            "Epoch 19/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.2909 - accuracy: 0.5995\n",
            "Epoch 20/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.2817 - accuracy: 0.6018\n",
            "Epoch 21/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2738 - accuracy: 0.6037\n",
            "Epoch 22/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.2642 - accuracy: 0.6059\n",
            "Epoch 23/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.2582 - accuracy: 0.6080\n",
            "Epoch 24/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2518 - accuracy: 0.6088\n",
            "Epoch 25/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2437 - accuracy: 0.6109\n",
            "Epoch 26/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2391 - accuracy: 0.6118\n",
            "Epoch 27/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2344 - accuracy: 0.6137\n",
            "Epoch 28/50\n",
            "4546/4546 [==============================] - 52s 11ms/step - loss: 1.2273 - accuracy: 0.6158\n",
            "Epoch 29/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2240 - accuracy: 0.6160\n",
            "Epoch 30/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2192 - accuracy: 0.6170\n",
            "Epoch 31/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2155 - accuracy: 0.6187\n",
            "Epoch 32/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2111 - accuracy: 0.6199\n",
            "Epoch 33/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2063 - accuracy: 0.6213\n",
            "Epoch 34/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.2031 - accuracy: 0.6211\n",
            "Epoch 35/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1989 - accuracy: 0.6230\n",
            "Epoch 36/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1952 - accuracy: 0.6232\n",
            "Epoch 37/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1923 - accuracy: 0.6244\n",
            "Epoch 38/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1883 - accuracy: 0.6253\n",
            "Epoch 39/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1869 - accuracy: 0.6258\n",
            "Epoch 40/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1832 - accuracy: 0.6274\n",
            "Epoch 41/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1792 - accuracy: 0.6282\n",
            "Epoch 42/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1777 - accuracy: 0.6282\n",
            "Epoch 43/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1741 - accuracy: 0.6285\n",
            "Epoch 44/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1729 - accuracy: 0.6293\n",
            "Epoch 45/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1706 - accuracy: 0.6298\n",
            "Epoch 46/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1680 - accuracy: 0.6309\n",
            "Epoch 47/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1647 - accuracy: 0.6311\n",
            "Epoch 48/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1640 - accuracy: 0.6321\n",
            "Epoch 49/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1615 - accuracy: 0.6318\n",
            "Epoch 50/50\n",
            "4546/4546 [==============================] - 51s 11ms/step - loss: 1.1596 - accuracy: 0.6320\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "model2.fit(x, y, \n",
        "          batch_size=batch_size, \n",
        "          epochs=epochs,\n",
        "          shuffle=True)\n",
        "model2.save_weights(model_file2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRm4OextEXH7"
      },
      "source": [
        "# > Model 3: **bidirectional RNN**\n",
        "consists of two LSTMs: one that processes the input sequence in a forward direction, and another that processes the input sequence in a backward direction. The output of each LSTM is fed to a dense layer, and the final output is a concatenation of the two dense layer outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLVSATlkEXH8",
        "outputId": "75edd88f-db2f-4fd2-dff0-112107ca95b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_2 (Bidirectio  (None, 256)              205824    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 72)                18504     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 224,328\n",
            "Trainable params: 224,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model3 = keras.Sequential(\n",
        "    [\n",
        "        Input(shape=(maxlen, len(chars))),\n",
        "        Bidirectional(LSTM(128)),  \n",
        "        Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model_file3 = 'NCP3_model.h5'\n",
        "\n",
        "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), #to minimize the categorical cross-entropy loss function (for multi-class classification)\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "print(model3.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RI_36JRSrq"
      },
      "source": [
        "**Adam**: it is an adaptive learning rate optimization algorithm, which means that it dynamically adjusts the learning rate of each weight parameter based on the gradient statistics computed during training.\n",
        "is used to minimize the categorical cross-entropy loss function, which is a common choice for multi-class classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE6zGZL8EXH8",
        "outputId": "8f8c5e19-310b-4f3b-e507-812af767251a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4546/4546 [==============================] - 43s 9ms/step - loss: 2.3090 - accuracy: 0.3372\n",
            "Epoch 2/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.9394 - accuracy: 0.4289\n",
            "Epoch 3/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.8026 - accuracy: 0.4648\n",
            "Epoch 4/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.7175 - accuracy: 0.4860\n",
            "Epoch 5/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.6557 - accuracy: 0.5036\n",
            "Epoch 6/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.6075 - accuracy: 0.5173\n",
            "Epoch 7/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.5685 - accuracy: 0.5284\n",
            "Epoch 8/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.5349 - accuracy: 0.5386\n",
            "Epoch 9/50\n",
            "4546/4546 [==============================] - 39s 8ms/step - loss: 1.5063 - accuracy: 0.5467\n",
            "Epoch 10/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.4820 - accuracy: 0.5535\n",
            "Epoch 11/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.4598 - accuracy: 0.5596\n",
            "Epoch 12/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.4400 - accuracy: 0.5651\n",
            "Epoch 13/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.4223 - accuracy: 0.5696\n",
            "Epoch 14/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.4062 - accuracy: 0.5739\n",
            "Epoch 15/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3920 - accuracy: 0.5776\n",
            "Epoch 16/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.3921 - accuracy: 0.5772\n",
            "Epoch 17/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.4252 - accuracy: 0.5673\n",
            "Epoch 18/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3756 - accuracy: 0.5813\n",
            "Epoch 19/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3617 - accuracy: 0.5854\n",
            "Epoch 20/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3512 - accuracy: 0.5880\n",
            "Epoch 21/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3399 - accuracy: 0.5909\n",
            "Epoch 22/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3297 - accuracy: 0.5937\n",
            "Epoch 23/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3202 - accuracy: 0.5960\n",
            "Epoch 24/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.3105 - accuracy: 0.5993\n",
            "Epoch 25/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.3022 - accuracy: 0.6011\n",
            "Epoch 26/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2939 - accuracy: 0.6036\n",
            "Epoch 27/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2860 - accuracy: 0.6056\n",
            "Epoch 28/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2782 - accuracy: 0.6073\n",
            "Epoch 29/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2705 - accuracy: 0.6095\n",
            "Epoch 30/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2634 - accuracy: 0.6113\n",
            "Epoch 31/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.2561 - accuracy: 0.6132\n",
            "Epoch 32/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2495 - accuracy: 0.6151\n",
            "Epoch 33/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2430 - accuracy: 0.6168\n",
            "Epoch 34/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2362 - accuracy: 0.6187\n",
            "Epoch 35/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2297 - accuracy: 0.6202\n",
            "Epoch 36/50\n",
            "4546/4546 [==============================] - 36s 8ms/step - loss: 1.2237 - accuracy: 0.6222\n",
            "Epoch 37/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2177 - accuracy: 0.6237\n",
            "Epoch 38/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.2112 - accuracy: 0.6259\n",
            "Epoch 39/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.2056 - accuracy: 0.6270\n",
            "Epoch 40/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1996 - accuracy: 0.6287\n",
            "Epoch 41/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1941 - accuracy: 0.6300\n",
            "Epoch 42/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.1883 - accuracy: 0.6319\n",
            "Epoch 43/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1827 - accuracy: 0.6334\n",
            "Epoch 44/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1773 - accuracy: 0.6351\n",
            "Epoch 45/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1722 - accuracy: 0.6365\n",
            "Epoch 46/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.1673 - accuracy: 0.6381\n",
            "Epoch 47/50\n",
            "4546/4546 [==============================] - 38s 8ms/step - loss: 1.1619 - accuracy: 0.6397\n",
            "Epoch 48/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1575 - accuracy: 0.6409\n",
            "Epoch 49/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1531 - accuracy: 0.6420\n",
            "Epoch 50/50\n",
            "4546/4546 [==============================] - 37s 8ms/step - loss: 1.1478 - accuracy: 0.6438\n",
            "4546/4546 [==============================] - 23s 5ms/step - loss: 1.1244 - accuracy: 0.6505\n",
            "Test loss: 1.1243928670883179\n",
            "Test accuracy: 0.6505255699157715\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "model3.fit(x, y, \n",
        "          batch_size=batch_size, \n",
        "          epochs=epochs,\n",
        "          shuffle=True)\n",
        "# Evaluate the model\n",
        "score = model3.evaluate(x, y, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "model3.save_weights(model_file3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asyqMi8nEXH9"
      },
      "source": [
        "> Model 4: GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdRRm-StEXH9",
        "outputId": "b6aaa45b-70fc-4bea-e86d-ba0f5d423026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 256)               253440    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 72)                18504     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 271,944\n",
            "Trainable params: 271,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model4 = keras.Sequential(\n",
        "    [\n",
        "        Input(shape=(maxlen, len(chars))),\n",
        "        GRU(256),\n",
        "        Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model_file4 = 'NCP4_model.h5'\n",
        "model4.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "print(model4.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nmaf6HgaEXH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23242e0-a5c9-4541-aa84-6c143877460d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4546/4546 [==============================] - 40s 7ms/step - loss: 1.9015 - accuracy: 0.4401\n",
            "Epoch 2/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.4915 - accuracy: 0.5507\n",
            "Epoch 3/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.3589 - accuracy: 0.5859\n",
            "Epoch 4/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.2869 - accuracy: 0.6048\n",
            "Epoch 5/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.2378 - accuracy: 0.6174\n",
            "Epoch 6/50\n",
            "4546/4546 [==============================] - 33s 7ms/step - loss: 1.2025 - accuracy: 0.6270\n",
            "Epoch 7/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.1741 - accuracy: 0.6342\n",
            "Epoch 8/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.1513 - accuracy: 0.6400\n",
            "Epoch 9/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.1318 - accuracy: 0.6455\n",
            "Epoch 10/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.1159 - accuracy: 0.6493\n",
            "Epoch 11/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.1013 - accuracy: 0.6534\n",
            "Epoch 12/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0896 - accuracy: 0.6564\n",
            "Epoch 13/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0800 - accuracy: 0.6589\n",
            "Epoch 14/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0711 - accuracy: 0.6613\n",
            "Epoch 15/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0632 - accuracy: 0.6636\n",
            "Epoch 16/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0566 - accuracy: 0.6652\n",
            "Epoch 17/50\n",
            "4546/4546 [==============================] - 31s 7ms/step - loss: 1.0512 - accuracy: 0.6670\n",
            "Epoch 18/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0452 - accuracy: 0.6677\n",
            "Epoch 19/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0414 - accuracy: 0.6693\n",
            "Epoch 20/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0366 - accuracy: 0.6706\n",
            "Epoch 21/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0330 - accuracy: 0.6712\n",
            "Epoch 22/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0298 - accuracy: 0.6723\n",
            "Epoch 23/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0266 - accuracy: 0.6729\n",
            "Epoch 24/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0237 - accuracy: 0.6739\n",
            "Epoch 25/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0215 - accuracy: 0.6737\n",
            "Epoch 26/50\n",
            "4546/4546 [==============================] - 31s 7ms/step - loss: 1.0196 - accuracy: 0.6749\n",
            "Epoch 27/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0173 - accuracy: 0.6753\n",
            "Epoch 28/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0159 - accuracy: 0.6763\n",
            "Epoch 29/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0131 - accuracy: 0.6764\n",
            "Epoch 30/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0115 - accuracy: 0.6768\n",
            "Epoch 31/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0105 - accuracy: 0.6766\n",
            "Epoch 32/50\n",
            "4546/4546 [==============================] - 33s 7ms/step - loss: 1.0087 - accuracy: 0.6777\n",
            "Epoch 33/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0063 - accuracy: 0.6782\n",
            "Epoch 34/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0057 - accuracy: 0.6780\n",
            "Epoch 35/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0033 - accuracy: 0.6794\n",
            "Epoch 36/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0021 - accuracy: 0.6793\n",
            "Epoch 37/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0014 - accuracy: 0.6790\n",
            "Epoch 38/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 1.0014 - accuracy: 0.6792\n",
            "Epoch 39/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9991 - accuracy: 0.6801\n",
            "Epoch 40/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9978 - accuracy: 0.6805\n",
            "Epoch 41/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9968 - accuracy: 0.6805\n",
            "Epoch 42/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9964 - accuracy: 0.6805\n",
            "Epoch 43/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9952 - accuracy: 0.6806\n",
            "Epoch 44/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9959 - accuracy: 0.6804\n",
            "Epoch 45/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9932 - accuracy: 0.6817\n",
            "Epoch 46/50\n",
            "4546/4546 [==============================] - 31s 7ms/step - loss: 0.9919 - accuracy: 0.6815\n",
            "Epoch 47/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9918 - accuracy: 0.6816\n",
            "Epoch 48/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9905 - accuracy: 0.6821\n",
            "Epoch 49/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9904 - accuracy: 0.6824\n",
            "Epoch 50/50\n",
            "4546/4546 [==============================] - 32s 7ms/step - loss: 0.9903 - accuracy: 0.6820\n",
            "4546/4546 [==============================] - 21s 5ms/step - loss: 0.9451 - accuracy: 0.6968\n",
            "Test loss: 0.9451407194137573\n",
            "Test accuracy: 0.6967730522155762\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "model4.fit(x, y, \n",
        "          batch_size=batch_size, \n",
        "          epochs=epochs,\n",
        "          shuffle=True,\n",
        "          verbose=1)\n",
        "# Evaluate the model\n",
        "score = model4.evaluate(x, y, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "model4.save_weights(model_file4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wUYCm6IEXH-"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=374496d5-c170-4e50-80a7-05bb8c49dee7' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "17267fb931fe4d6d97cded38c031812b",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}